---
title: "Bioinformatics Lecture 9 Mini-Project"
author: "Corrina Elder"
date: "May 2, 2018"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Section 1

Let's read our input data:

```{r}
url <- "https://bioboot.github.io/bggn213_S18/class-material/WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(url)
head(wisc.df)
```

How many diagnosis are cancer (M) vs non cancer (B)

```{r}
table(wisc.df$diagnosis)
```

Remove id and diagnosis columns

```{r}
# Convert the features of the data: wisc.data
#Also remove the problem 'X' column in position 33
wisc.data <- as.matrix(wisc.df[,-c(1:2, 33)])

# Set the row names of wisc.data
row.names(wisc.data) <- wisc.df$id
#head(wisc.data)
```


Finally, setup a separate new vector called diagnosis to be 1 if a diagnosis is malignant ("M") and 0 otherwise. Note that R coerces TRUE to 1 and FALSE to 0.

```{r}
# Create diagnosis vector by completing the missing code
diagnosis <- as.numeric(wisc.df$diagnosis == "M")
```

Explore the data you created before (wisc.data and diagnosis) to answer the following questions:

Q1. How many observations are in this dataset? 
    wisc.data has 569 observations (use nrow(wisc.data)).
Q2. How many variables/features in the data are suffixed with _mean? 
    dim(wisc.data) gives 31 columns
    
```{r}
length(grep("_mean", colnames(wisc.data)))
```
    
Q3. How many of the observations have a malignant diagnosis?
    212 (table(wisc.df))

# Section 2

The next step in your analysis is to perform principal component analysis (PCA) on  wisc.data.

It is important to check if the data need to be scaled before performing PCA. Recall two common reasons for scaling data include:

The input variables use different units of measurement.
The input variables have significantly different variances.
Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to determine if the data should be scaled. Use the colMeans() and apply() functions like youâ€™ve done before.

```{r}
# Check column means and standard deviations
plot(colMeans(wisc.data), type = "h")

apply(wisc.data, 2, sd)
```


Execute PCA with the prcomp() function on the wisc.data, scaling if appropriate, and assign the output model to wisc.pr.

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale = TRUE)
```

```{r}
# Look at summary of results
summary(wisc.pr)
```

Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
    0.4427 (PC1 Proportion of Variance)
Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
    3 (Cumulative Proportion)
Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
    7 (Cumulative Proportion)
    
```{r}
biplot(wisc.pr)
```    
Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
    It's difficult to read because of the names and the points are not well spread out.

Scatter plot
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis + 1)
```

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[, c(1, 3)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC3")
```

Variance explained

```{r}
# Variance explained by each principal component: pve
pve <- wisc.pr$sdev^2 / sum(wisc.pr$sdev^2)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
# Barplot
barplot(pve, names.arg = paste("PC", 1:length(pve)), las = 2, axes = FALSE, ylab = "Proportion of Variance")
axis(2, at = pve, labels = round(pve, 2) * 100)
```

```{r}
# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Plot cumulative proportion of variance explained
# plot( ___ , xlab = "Principal Component", 
#      ylab = "Cumulative Proportion of Variance Explained", 
#      ylim = c(0, 1), type = "o")
```

Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?


# Section 3
Hierarchical clustering of case data

```{r}
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)

data.dist <- dist(data.scaled)

wisc.hclust <- hclust(data.dist, method = "complete")

plot(wisc.hclust)

abline(h = 20, col = "red", lwd = 3)
```


```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)

table(wisc.hclust.clusters, diagnosis)
```

Q12. Can you find a better cluster vs diagnoses match with by cutting into a different number of clusters between 2 and 10?

No. Fewer clusters loses distinction, more splits it too much.

```{r}
#wisc.hclust.clusters <- cutree(wisc.hclust, k = 5)

#table(wisc.hclust.clusters, diagnosis)
```


# Section 4
K-means clustering and comparing results

```{r}
wisc.km <- kmeans(wisc.data, centers = 2, nstart = 20)

table(wisc.km$cluster, diagnosis)
```
Q13. How well does k-means separate the two diagnoses? How does it compare to your hclust results?
     It does a good job separating benign, but the malignant is not as good as hclust.
     
```{r}
table(wisc.hclust.clusters, wisc.km$cluster)
```
     

# Section 5
Clustering on PCA results

```{r}
# Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:3]), method = "ward.D2")

plot(wisc.pr.hclust)

wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k = 2)

table(wisc.pr.hclust.clusters, diagnosis)

plot(wisc.pr$x[, 1:2], col = wisc.pr.hclust.clusters)
```


```{r}
#install.packages("rgl")
```

```{r}
#plot3d(wisc.pr$x[, 1:3], type = "s", col = wisc.pr.hclust.clusters)
```


```{r}
#url <- "https://tinyurl.com/new-samples-CSV"
#new <- read.csv(url)
#npc <- predict(wisc.pr, newdata = new)

#plot(wisc.pr$x[ , 1:2], col = wisc.pr.hclust.clusters)
#points(npc[ , 1], npc[ , 2], col = c("blue", "purple"), pch = 16, cex = 2)
```






